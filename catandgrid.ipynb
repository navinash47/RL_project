{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from catvsmonsters import catVsMonsters\n",
    "from grid_world import GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "class Node:\n",
    "    def __init__(self, state, parent=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.visits = 0\n",
    "        self.value = 0.0\n",
    "        \n",
    "    def get_child_ucb1(self, child, c):\n",
    "        if child.visits == 0:\n",
    "            return float(\"inf\")\n",
    "        return child.value / child.visits + c * np.sqrt(np.log(self.visits) / child.visits)\n",
    "        \n",
    "    def get_max_ucb1_child(self, c):\n",
    "        if not self.children:\n",
    "            return None, None\n",
    "            \n",
    "        max_i = 0\n",
    "        max_ucb1 = float(\"-inf\")\n",
    "        \n",
    "        for i, child in enumerate(self.children):\n",
    "            ucb1 = self.get_child_ucb1(child, c)\n",
    "            \n",
    "            if ucb1 > max_ucb1:\n",
    "                max_ucb1 = ucb1\n",
    "                max_i = i\n",
    "                \n",
    "        return self.children[max_i], max_i\n",
    "\n",
    "class MonteCarloSearchTree:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.root = Node((0,0))\n",
    "        self.depth_limit = 200\n",
    "        self.c = 100  # Changed from 100 to standard UCT constant\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    def _selection(self, node):\n",
    "        while len(node.children) > 0:\n",
    "            child, _ = node.get_max_ucb1_child(self.c)\n",
    "            node = child\n",
    "        return node\n",
    "\n",
    "    def _expansion(self, node):\n",
    "        for action in self.env.actions:\n",
    "            next_state = self.env.get_next_state(node.state, action)\n",
    "            child = Node(next_state, parent=node)\n",
    "            node.children.append(child)\n",
    "        return node\n",
    "\n",
    "    def _simulation(self, node):\n",
    "        # Save the original state\n",
    "        original_state = self.env.current_state\n",
    "        \n",
    "        # Set environment to node's state\n",
    "        self.env.current_state = node.state\n",
    "        current_state = node.state\n",
    "        total_reward = 0\n",
    "        depth = 0\n",
    "        \n",
    "        while current_state not in [self.env.goal] and depth < self.depth_limit:\n",
    "            action = np.random.choice(self.env.actions)\n",
    "            next_state, reward, done = self.env.step(action)\n",
    "            total_reward += (self.gamma ** depth) * reward\n",
    "            current_state = next_state\n",
    "            depth += 1\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Restore the original state\n",
    "        self.env.current_state = original_state\n",
    "        return total_reward\n",
    "\n",
    "    def _backpropagation(self, node, reward):\n",
    "        while node is not None:\n",
    "            node.visits += 1\n",
    "            # Use average instead of cumulative sum\n",
    "            node.value = (node.value * (node.visits - 1) + reward) / node.visits\n",
    "            # node.value += reward\n",
    "            reward = reward * self.gamma\n",
    "            node = node.parent\n",
    "\n",
    "    def get_best_action(self, iterations, min_visits=1000):\n",
    "        \n",
    "        def run_mcts():\n",
    "            if self.root.children != []:\n",
    "                print(f'child.visits: {self.root.children[0].visits}')\n",
    "            for _ in range(iterations):\n",
    "                leaf = self._selection(self.root)\n",
    "                if leaf.state == self.env.goal:\n",
    "                    continue\n",
    "                if leaf.visits == 0:\n",
    "                    leaf = self._expansion(leaf)\n",
    "                simulation_result = self._simulation(leaf)\n",
    "                self._backpropagation(leaf, simulation_result)\n",
    "        \n",
    "        while True:\n",
    "            run_mcts()\n",
    "            if all(child.visits >= min_visits for child in self.root.children):\n",
    "                break\n",
    "\n",
    "            \n",
    "        # When calculating final values, ensure we're using the average\n",
    "        values = [float('-inf')] * len(self.env.actions)\n",
    "        for i, child in enumerate(self.root.children):\n",
    "            if child.visits > 0:\n",
    "                print(f'child.value: {child.value}, child.visits: {child.visits}')\n",
    "                values[i] = child.value\n",
    "            else:\n",
    "                values[i] = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        return values, max(values), np.argmax(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:00<00:02,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 500, optimal_value: -inf, optimal_action: 0\n",
      "iteration: 500, optimal_value: -inf, optimal_action: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:01<00:01,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 500, optimal_value: -inf, optimal_action: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:02<00:00,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 500, optimal_value: -inf, optimal_action: 0\n",
      "iteration: 500, optimal_value: -inf, optimal_action: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:03<00:00,  1.66it/s]\n",
      " 20%|██        | 1/5 [00:01<00:04,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1000, optimal_value: -inf, optimal_action: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:02<00:03,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1000, optimal_value: -inf, optimal_action: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:04<00:02,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1000, optimal_value: -inf, optimal_action: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:05<00:01,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1000, optimal_value: -inf, optimal_action: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:07<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1000, optimal_value: -inf, optimal_action: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:14<00:59, 14.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 10000, optimal_value: -inf, optimal_action: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:31<00:47, 15.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 10000, optimal_value: -inf, optimal_action: 0\n"
     ]
    }
   ],
   "source": [
    "def plot_graph_for_test(env,min_visits=1000):\n",
    "    mcts = MonteCarloSearchTree(env)\n",
    "    mins={}\n",
    "    maxs={}\n",
    "    avgs={}\n",
    "    iterations=[500,1000,10000,100000]\n",
    "    # time_slots=[1,5,10,50,100]\n",
    "    for iteration in iterations:\n",
    "        min_value = float('inf')\n",
    "        max_value = float('-inf')\n",
    "        avg_value = 0\n",
    "        avg_iteration = 0\n",
    "        for i in tqdm(range(5)):\n",
    "            # env.reset()\n",
    "            mcts.root = Node((0,4))\n",
    "            probs_pi,optimal_value,optimal_action = mcts.get_best_action(iteration,min_visits=min_visits)\n",
    "            print(f'iteration: {iteration}, optimal_value: {optimal_value+6}, optimal_action: {optimal_action}')\n",
    "            min_value = min(min_value, optimal_value)\n",
    "            max_value = max(max_value, optimal_value)\n",
    "            avg_value += optimal_value\n",
    "        avg_value /= 5\n",
    "        mins[iteration] = min_value\n",
    "        maxs[iteration] = max_value\n",
    "        avgs[iteration] = avg_value\n",
    "    \n",
    "    return mins,maxs,avgs,iterations\n",
    "\n",
    "\n",
    "cat_dynamics = catVsMonsters()\n",
    "# cat_dynamics.current_state = (0,4)\n",
    "mins_cat,maxs_cat, avgs_cat, iterations_cat = plot_graph_for_test(cat_dynamics,min_visits=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:01<00:04,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "child.value: -3.814399801947498, child.visits: 125\n",
      "child.value: -4.260777201702612, child.visits: 124\n",
      "child.value: -3.315671392312747, child.visits: 125\n",
      "child.value: -3.7295268008528346, child.visits: 125\n",
      "iteration: 500, optimal_value: -3.315671392312747, optimal_action: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:03<00:01,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "child.value: -4.209917192329113, child.visits: 125\n",
      "child.value: -3.5079612925614114, child.visits: 125\n",
      "child.value: -3.311218419922749, child.visits: 125\n",
      "child.value: -4.940803725088387, child.visits: 124\n",
      "iteration: 500, optimal_value: -3.311218419922749, optimal_action: 2\n",
      "child.value: -3.933572401274527, child.visits: 125\n",
      "child.value: -4.545344322409392, child.visits: 125\n",
      "child.value: -4.24745674633098, child.visits: 125\n",
      "child.value: -4.671081031855238, child.visits: 124\n",
      "iteration: 500, optimal_value: -3.933572401274527, optimal_action: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:03<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "child.value: -3.821802054096597, child.visits: 125\n",
      "child.value: -3.6891946673470124, child.visits: 125\n",
      "child.value: -4.347180464314795, child.visits: 125\n",
      "child.value: -4.776524807723827, child.visits: 124\n",
      "iteration: 500, optimal_value: -3.6891946673470124, optimal_action: 1\n",
      "child.value: -3.6541282833969726, child.visits: 125\n",
      "child.value: -4.408422807511806, child.visits: 125\n",
      "child.value: -3.114189841852566, child.visits: 125\n",
      "child.value: -4.7450667799653115, child.visits: 124\n",
      "iteration: 500, optimal_value: -3.114189841852566, optimal_action: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:04<00:00,  1.07it/s]\n",
      " 20%|██        | 1/5 [00:01<00:07,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "child.value: -3.157556169029629, child.visits: 250\n",
      "child.value: -4.003453147552813, child.visits: 249\n",
      "child.value: -3.478421819171639, child.visits: 250\n",
      "child.value: -3.7781519941433, child.visits: 250\n",
      "iteration: 1000, optimal_value: -3.157556169029629, optimal_action: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:03<00:14,  3.52s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/numpy/core/fromnumeric.py:77\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'prod'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m cat_dynamics \u001b[38;5;241m=\u001b[39m catVsMonsters()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# cat_dynamics.current_state = (0,0)\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m mins_cat,maxs_cat, avgs_cat, iterations_cat \u001b[38;5;241m=\u001b[39m \u001b[43mplot_graph_for_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat_dynamics\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmin_visits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m, in \u001b[0;36mplot_graph_for_test\u001b[0;34m(env, min_visits)\u001b[0m\n\u001b[1;32m     14\u001b[0m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     15\u001b[0m mcts\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m Node((\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m---> 16\u001b[0m probs_pi,optimal_value,optimal_action \u001b[38;5;241m=\u001b[39m \u001b[43mmcts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmin_visits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_visits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miteration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, optimal_value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimal_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, optimal_action: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimal_action\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m min_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(min_value, optimal_value)\n",
      "Cell \u001b[0;32mIn[2], line 92\u001b[0m, in \u001b[0;36mMonteCarloSearchTree.get_best_action\u001b[0;34m(self, iterations, min_visits)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m leaf\u001b[38;5;241m.\u001b[39mvisits \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     91\u001b[0m         leaf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expansion(leaf)\n\u001b[0;32m---> 92\u001b[0m     simulation_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleaf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backpropagation(leaf, simulation_result)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# run_mcts()\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# When calculating final values, ensure we're using the average\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 64\u001b[0m, in \u001b[0;36mMonteCarloSearchTree._simulation\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m     61\u001b[0m depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m current_state \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mgoal] \u001b[38;5;129;01mand\u001b[39;00m depth \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth_limit:\n\u001b[0;32m---> 64\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     next_state, reward, done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     66\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m depth) \u001b[38;5;241m*\u001b[39m reward\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:998\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:782\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mnumpy/random/_bounded_integers.pyx:1315\u001b[0m, in \u001b[0;36mnumpy.random._bounded_integers._rand_int64\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/numpy/core/fromnumeric.py:3100\u001b[0m, in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2979\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_prod_dispatcher)\n\u001b[1;32m   2980\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprod\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2981\u001b[0m          initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   2982\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2983\u001b[0m \u001b[38;5;124;03m    Return the product of array elements over a given axis.\u001b[39;00m\n\u001b[1;32m   2984\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3098\u001b[0m \u001b[38;5;124;03m    10\u001b[39;00m\n\u001b[1;32m   3099\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprod\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3101\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/numpy/core/fromnumeric.py:77\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mu\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m         reduction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def plot_graph_for_test(env,min_visits=1000):\n",
    "    mcts = MonteCarloSearchTree(env)\n",
    "    mins={}\n",
    "    maxs={}\n",
    "    avgs={}\n",
    "    iterations=[500,1000,10000,100000]\n",
    "    # time_slots=[1,5,10,50,100]\n",
    "    for iteration in iterations:\n",
    "        min_value = float('inf')\n",
    "        max_value = float('-inf')\n",
    "        avg_value = 0\n",
    "        avg_iteration = 0\n",
    "        for i in tqdm(range(5)):\n",
    "            env.reset()\n",
    "            mcts.root = Node((0,0))\n",
    "            probs_pi,optimal_value,optimal_action = mcts.get_best_action(iteration,min_visits=min_visits)\n",
    "            print(f'iteration: {iteration}, optimal_value: {optimal_value}, optimal_action: {optimal_action}')\n",
    "            min_value = min(min_value, optimal_value)\n",
    "            max_value = max(max_value, optimal_value)\n",
    "            avg_value += optimal_value\n",
    "        avg_value /= 5\n",
    "        mins[iteration] = min_value\n",
    "        maxs[iteration] = max_value\n",
    "        avgs[iteration] = avg_value\n",
    "    \n",
    "    return mins,maxs,avgs,iterations\n",
    "\n",
    "\n",
    "cat_dynamics = catVsMonsters()\n",
    "# cat_dynamics.current_state = (0,0)\n",
    "mins_cat,maxs_cat, avgs_cat, iterations_cat = plot_graph_for_test(cat_dynamics,min_visits=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# # plot with time slots\n",
    "# #  and range  in lighter blue and avg in darker blue\n",
    "# x_values = [500,1000,10000,100000]\n",
    "# y_values =list(avgs_cat.values())\n",
    "# plt.plot(x_values,y_values,label='avg')\n",
    "# plt.fill_between(x_values,list(mins_cat.values()),list(maxs_cat.values()),alpha=0.5)\n",
    "# plt.legend()\n",
    "# # plt.xscale('log')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "env_cat = catVsMonsters()\n",
    "mcts_cat = MonteCarloSearchTree(env_cat)\n",
    "probs_pi_cat={}\n",
    "pi_cat={}\n",
    "for k in tqdm(range(25)):\n",
    "    i = k // 5\n",
    "    j = k % 5\n",
    "    action_count = [0,0,0,0]\n",
    "    for p in range(5):\n",
    "        if (i,j) not in env_cat.furniture and (i,j) != env_cat.goal:\n",
    "            env_cat.current_state = (i,j)\n",
    "            mcts_cat.root = Node((i,j))\n",
    "            probs_pi_cat[(i,j)],optimal_value,optimal_action = mcts_cat.get_best_action(20000,min_visits=1000)\n",
    "            print(f'state: {(i,j)}, optimal_value: {optimal_value}, optimal_action: {optimal_action}')\n",
    "            action_count[optimal_action] += 1\n",
    "    if (i,j) not in env_cat.furniture and (i,j) != env_cat.goal:\n",
    "        pi_cat[(i,j)] = action_count.index(max(action_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_world = GridWorld()\n",
    "mins_grid,maxs_grid, avgs_grid, iterations_grid = plot_graph_for_test(grid_world)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plot with time slots\n",
    "#  and range  in lighter blue and avg in darker blue\n",
    "x_values = [10,50,100,500,1000,10000,100000]\n",
    "y_values =list(avgs_grid.values())\n",
    "plt.plot(x_values,y_values,label='avg')\n",
    "plt.fill_between(x_values,list(mins_grid.values()),list(maxs_grid.values()),alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "env = GridWorld()\n",
    "mcts = MonteCarloSearchTree(env)\n",
    "probs_pi={}\n",
    "pi={}\n",
    "for k in tqdm(range(25)):\n",
    "    i = k // 5\n",
    "    j = k % 5\n",
    "    action_count = [0,0,0,0]\n",
    "    for p in range(5):\n",
    "        if (i,j) not in env.obstacles and (i,j) != env.goal:\n",
    "            env.current_state = (i,j)\n",
    "            mcts.root = Node((i,j))\n",
    "            probs_pi[(i,j)],optimal_value,optimal_action = mcts.get_best_action(20000)\n",
    "            print(f'state: {(i,j)}, optimal_value: {optimal_value}, optimal_action: {optimal_action}')\n",
    "            action_count[optimal_action] += 1\n",
    "    if (i,j) not in env.obstacles and (i,j) != env.goal:\n",
    "        pi[(i,j)] = action_count.index(max(action_count))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(policy):\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            state = (i, j)\n",
    "            if state == env.goal:\n",
    "                print(\"G\", end=\" \")\n",
    "                continue\n",
    "            elif state in env.obstacles:\n",
    "                print(\"O\", end=\" \")\n",
    "                continue\n",
    "            action = policy[state]\n",
    "            if action == 0:\n",
    "                print(\"↑\", end=\" \")\n",
    "            elif action == 1:\n",
    "                print(\"↓\", end=\" \")\n",
    "            elif action == 2:\n",
    "                print(\"←\", end=\" \")\n",
    "            elif action == 3:\n",
    "                print(\"→\", end=\" \")\n",
    "            else:\n",
    "                print(\" \", end=\" \")\n",
    "        print()\n",
    "\n",
    "print_policy(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_greedy_policy(probs_pi):\n",
    "    policy = {}\n",
    "    for state in probs_pi:\n",
    "        if state not in env.obstacles and state != env.goal:\n",
    "            policy[state] = np.argmax(probs_pi[state])\n",
    "    return policy\n",
    "print(probs_pi)\n",
    "print_policy(get_greedy_policy(probs_pi))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
