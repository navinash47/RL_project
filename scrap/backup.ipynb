{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# from tqdm import tqdm\n",
    "# class Node:\n",
    "#     def __init__(self, state, parent=None):\n",
    "#         self.state = state\n",
    "#         self.parent = parent\n",
    "#         self.children = []\n",
    "#         self.visits = 0\n",
    "#         self.value = 0.0  # Initialize as float\n",
    "        \n",
    "\n",
    "# class MonteCarloSearchTree:\n",
    "#     def __init__(self,env):\n",
    "#         self.env = env\n",
    "#         self.root = Node((0,0))\n",
    "#         self.depth_limit = 200 # Added depth_limit as class attribute\n",
    "#         self.c = 1.414\n",
    "#         self.gamma = 0.9\n",
    "\n",
    "#     def _selection(self,node):\n",
    "#         while len(node.children) > 0:\n",
    "#             # Check for unexplored children first\n",
    "#             unexplored = [c for c in node.children if c.visits == 0]\n",
    "#             if unexplored:\n",
    "#                 return np.random.choice(unexplored)\n",
    "#             # If all children explored, use UCB\n",
    "#             ucb_values = [self._ucb(child, node.visits) for child in node.children]\n",
    "#             node = max(node.children, key=lambda x: ucb_values[node.children.index(x)])\n",
    "#         return node\n",
    "\n",
    "#     def _ucb(self, child, parent_visits):\n",
    "#         if child.visits == 0:\n",
    "#             return float('inf')  # Ensure unvisited nodes are explored\n",
    "#         exploitation = child.value / child.visits\n",
    "#         exploration = self.c * np.sqrt(np.log(parent_visits) / child.visits)\n",
    "#         return exploitation + exploration\n",
    "\n",
    "#     def _expansion(self, node):\n",
    "#         # Current implementation might not be exploring all actions\n",
    "#         unexplored_actions = [a for a in self.env.actions if not any(c.state == self.env.get_next_state(node.state, a) for c in node.children)]\n",
    "#         if unexplored_actions:\n",
    "#             action = np.random.choice(unexplored_actions)  # Randomize the choice\n",
    "#             next_state = self.env.get_next_state(node.state, action)\n",
    "#             child = Node(next_state, parent=node)\n",
    "#             node.children.append(child)\n",
    "#             return child\n",
    "#         return node\n",
    "\n",
    "#     def _simulation(self,node):\n",
    "#         #simulating node\n",
    "#         current_state = node.state\n",
    "#         total_reward = 0\n",
    "#         depth = 0\n",
    "        \n",
    "#         while current_state not in [self.env.goal] and depth < self.depth_limit:\n",
    "#             action = np.random.choice(self.env.actions)\n",
    "#             next_state, reward, done = self.env.step(action)\n",
    "#             total_reward += (self.gamma ** depth) * reward\n",
    "#             current_state = next_state\n",
    "#             depth += 1\n",
    "#             if done:\n",
    "#                 break\n",
    "            \n",
    "#         return total_reward\n",
    "\n",
    "#     def _backpropagation(self,node,reward):\n",
    "#         while node is not None:\n",
    "#             node.visits += 1\n",
    "#             # Update to use average values instead of cumulative\n",
    "#             node.value = (node.value * (node.visits - 1) + reward) / node.visits\n",
    "#             reward = reward * self.gamma\n",
    "#             node = node.parent\n",
    "\n",
    "#     def get_best_action(self, iterations):\n",
    "#         for _ in range(iterations):\n",
    "#             leaf = self._selection(self.root)\n",
    "#             if leaf.state == self.env.goal:\n",
    "#                 continue\n",
    "#             if leaf.visits == 0 or len(leaf.children) < len(self.env.actions):\n",
    "#                 leaf = self._expansion(leaf)\n",
    "#             simulation_result = self._simulation(leaf)\n",
    "#             self._backpropagation(leaf, simulation_result)\n",
    "        \n",
    "#         # Calculate action probabilities\n",
    "#         values = []\n",
    "#         for action in self.env.actions:\n",
    "#             child = next((c for c in self.root.children if c.state == self.env.get_next_state(self.root.state, action)), None)\n",
    "#             if child and child.visits > 0:\n",
    "#                 values.append(child.value / child.visits)\n",
    "#             else:\n",
    "#                 values.append(float('-inf'))\n",
    "#         print(values)\n",
    "        \n",
    "#         return values, max(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# from tqdm import tqdm\n",
    "# class Node:\n",
    "#     def __init__(self, state, parent=None):\n",
    "#         self.state = state\n",
    "#         self.parent = parent\n",
    "#         self.children = []\n",
    "#         self.visits = 0\n",
    "#         self.value = 0.0\n",
    "\n",
    "# class MonteCarloSearchTree:\n",
    "#     def __init__(self, env):\n",
    "#         self.env = env\n",
    "#         self.root = Node((0,0))\n",
    "#         self.depth_limit = 200\n",
    "#         self.c = 1.414\n",
    "#         self.gamma = 0.9\n",
    "\n",
    "#     def _selection(self, node):\n",
    "#         while len(node.children) > 0:\n",
    "#             ucb_values = [self._ucb(child, node.visits) for child in node.children]\n",
    "#             node = node.children[np.argmax(ucb_values)]\n",
    "#         return node\n",
    "\n",
    "#     def _ucb(self, child, parent_visits):\n",
    "#         if child.visits == 0:\n",
    "#             return float('inf')\n",
    "#         exploitation = child.value / child.visits\n",
    "#         exploration = self.c * np.sqrt(np.log(parent_visits) / child.visits)\n",
    "#         return exploitation + exploration\n",
    "\n",
    "#     def _expansion(self, node):\n",
    "#         for action in self.env.actions:\n",
    "#             next_state = self.env.get_next_state(node.state, action)\n",
    "#             child = Node(next_state, parent=node)\n",
    "#             node.children.append(child)\n",
    "#         return node\n",
    "\n",
    "#     def _simulation(self, node):\n",
    "#         current_state = node.state\n",
    "#         total_reward = 0\n",
    "#         depth = 0\n",
    "        \n",
    "#         while current_state not in [self.env.goal] and depth < self.depth_limit:\n",
    "#             action = np.random.choice(self.env.actions)\n",
    "#             next_state, reward, done = self.env.step(action)\n",
    "#             total_reward += (self.gamma ** depth) * reward\n",
    "#             current_state = next_state\n",
    "#             depth += 1\n",
    "#             if done:\n",
    "#                 break\n",
    "            \n",
    "#         return total_reward\n",
    "\n",
    "#     def _backpropagation(self, node, reward):\n",
    "#         while node is not None:\n",
    "#             node.visits += 1\n",
    "#             # Update to use average values instead of cumulative\n",
    "#             node.value = (node.value * (node.visits - 1) + reward) / node.visits\n",
    "#             reward = reward * self.gamma\n",
    "#             node = node.parent\n",
    "\n",
    "#     def get_best_action(self, iterations):\n",
    "#         for _ in range(iterations):\n",
    "#             leaf = self._selection(self.root)\n",
    "#             if leaf.state == self.env.goal:\n",
    "#                 continue\n",
    "#             if leaf.visits == 0:\n",
    "#                 leaf = self._expansion(leaf)\n",
    "#             simulation_result = self._simulation(leaf)\n",
    "#             self._backpropagation(leaf, simulation_result)\n",
    "        \n",
    "#         values = []\n",
    "#         for action in self.env.actions:\n",
    "#             child = next((c for c in self.root.children if c.state == self.env.get_next_state(self.root.state, action)), None)\n",
    "#             if child and child.visits > 0:\n",
    "#                 values.append(child.value / child.visits)\n",
    "#             else:\n",
    "#                 values.append(float('-inf'))\n",
    "#         print(values)\n",
    "        \n",
    "#         return values, max(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# from tqdm import tqdm\n",
    "# class Node:\n",
    "#     def __init__(self, state, parent=None):\n",
    "#         self.state = state\n",
    "#         self.parent = parent\n",
    "#         self.children = []\n",
    "#         self.visits = 0\n",
    "#         self.value = 0.0\n",
    "\n",
    "# class MonteCarloSearchTree:\n",
    "#     def __init__(self, env):\n",
    "#         self.env = env\n",
    "#         self.root = Node((0,0))\n",
    "#         self.depth_limit = 200\n",
    "#         self.c = 1.414\n",
    "#         self.gamma = 0.9\n",
    "\n",
    "#     def _selection(self, node):\n",
    "#         while len(node.children) > 0:\n",
    "#             ucb_values = [self._ucb(child, node.visits) for child in node.children]\n",
    "#             node = node.children[np.argmax(ucb_values)]\n",
    "#         return node\n",
    "\n",
    "#     def _ucb(self, child, parent_visits):\n",
    "#         if child.visits == 0:\n",
    "#             return float('inf')\n",
    "#         exploitation = child.value / child.visits\n",
    "#         exploration = self.c * np.sqrt(np.log(parent_visits) / child.visits)\n",
    "#         return exploitation + exploration\n",
    "\n",
    "#     def _expansion(self, node):\n",
    "#         for action in self.env.actions:\n",
    "#             next_state = self.env.get_next_state(node.state, action)\n",
    "#             child = Node(next_state, parent=node)\n",
    "#             node.children.append(child)\n",
    "#         return node\n",
    "\n",
    "#     def _simulation(self, node):\n",
    "#         # Save the original state\n",
    "#         original_state = self.env.current_state\n",
    "        \n",
    "#         # Set environment to node's state\n",
    "#         self.env.current_state = node.state\n",
    "#         current_state = node.state\n",
    "#         total_reward = 0\n",
    "#         depth = 0\n",
    "        \n",
    "#         while current_state not in [self.env.goal] and depth < self.depth_limit:\n",
    "#             action = np.random.choice(self.env.actions)\n",
    "#             next_state, reward, done = self.env.step(action)\n",
    "#             total_reward += (self.gamma ** depth) * reward\n",
    "#             current_state = next_state\n",
    "#             depth += 1\n",
    "#             if done:\n",
    "#                 break\n",
    "        \n",
    "#         # Restore the original state\n",
    "#         self.env.current_state = original_state\n",
    "#         return total_reward\n",
    "\n",
    "#     def _backpropagation(self, node, reward):\n",
    "#         while node is not None:\n",
    "#             node.visits += 1\n",
    "#             # Update to use average values instead of cumulative\n",
    "#             node.value = (node.value * (node.visits - 1) + reward) / node.visits\n",
    "#             reward = reward * self.gamma\n",
    "#             node = node.parent\n",
    "\n",
    "#     def get_best_action(self, iterations):\n",
    "#         for _ in range(iterations):\n",
    "#             leaf = self._selection(self.root)\n",
    "#             if leaf.state == self.env.goal:\n",
    "#                 continue\n",
    "#             if leaf.visits == 0:\n",
    "#                 leaf = self._expansion(leaf)\n",
    "#             simulation_result = self._simulation(leaf)\n",
    "#             self._backpropagation(leaf, simulation_result)\n",
    "        \n",
    "#         # Calculate values for each action\n",
    "#         values = []\n",
    "#         for action in self.env.actions:\n",
    "#             next_state = self.env.get_next_state(self.root.state, action)\n",
    "#             child = next((c for c in self.root.children if c.state == next_state), None)\n",
    "#             if child and child.visits > 0:\n",
    "#                 values.append(child.value / child.visits)\n",
    "#             else:\n",
    "#                 values.append(float('-inf'))\n",
    "        \n",
    "#         return values, max(values), np.argmax(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "class Node:\n",
    "    def __init__(self, state, parent=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.visits = 0\n",
    "        self.value = 0.0\n",
    "\n",
    "class MonteCarloSearchTree:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.root = Node((0,0))\n",
    "        self.depth_limit = 200\n",
    "        self.c = 2\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    def _selection(self, node):\n",
    "        while len(node.children) > 0:\n",
    "            ucb_values = [self._ucb(child, node.visits) for child in node.children]\n",
    "            node = node.children[np.argmax(ucb_values)]\n",
    "        return node\n",
    "\n",
    "    def _ucb(self, child, parent_visits):\n",
    "        if child.visits == 0:\n",
    "            return float('inf')\n",
    "        exploitation = child.value / child.visits\n",
    "        exploration = self.c * np.sqrt(np.log(parent_visits) / child.visits)\n",
    "        return exploitation + exploration\n",
    "\n",
    "    def _expansion(self, node):\n",
    "        for action in self.env.actions:\n",
    "            next_state = self.env.get_next_state(node.state, action)\n",
    "            child = Node(next_state, parent=node)\n",
    "            node.children.append(child)\n",
    "        return node\n",
    "\n",
    "    def _simulation(self, node):\n",
    "        # Save the original state\n",
    "        original_state = self.env.current_state\n",
    "        \n",
    "        # Set environment to node's state\n",
    "        self.env.current_state = node.state\n",
    "        current_state = node.state\n",
    "        total_reward = 0\n",
    "        depth = 0\n",
    "        \n",
    "        while current_state not in [self.env.goal] and depth < self.depth_limit:\n",
    "            action = np.random.choice(self.env.actions)\n",
    "            next_state, reward, done = self.env.step(action)\n",
    "            total_reward += (self.gamma ** depth) * reward\n",
    "            current_state = next_state\n",
    "            depth += 1\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Restore the original state\n",
    "        self.env.current_state = original_state\n",
    "        return total_reward\n",
    "\n",
    "    def _backpropagation(self, node, reward):\n",
    "        while node is not None:\n",
    "            node.visits += 1\n",
    "            # Update to use average values instead of cumulative\n",
    "            node.value = (node.value * (node.visits - 1) + reward) / node.visits\n",
    "            reward = reward * self.gamma\n",
    "            node = node.parent\n",
    "\n",
    "    def get_best_action(self, iterations,min_visits=1):\n",
    "        iteration_count = 0\n",
    "        visits_completed= False\n",
    "        \n",
    "        # while True:\n",
    "        #     leaf = self._selection(self.root)\n",
    "        #     if leaf.state == self.env.goal:\n",
    "        #         self._backpropagation(leaf, 0)\n",
    "        #     if leaf.visits == 0:\n",
    "        #         leaf = self._expansion(leaf)\n",
    "        #     simulation_result = self._simulation(leaf)\n",
    "        #     self._backpropagation(leaf, simulation_result)\n",
    "        #     iteration_count += 1\n",
    "            \n",
    "        #     min_v = min(self.root.children, key=lambda x: x.visits).visits\n",
    "            \n",
    "        #     if iteration_count % 1000 == 0:\n",
    "        #         print(f'iteration_count: {iteration_count}, min_v: {min_v}')\n",
    "        #         pass\n",
    "        #     # find the minimum value of the children\n",
    "        #     if min_v >= min_visits:\n",
    "        #         print(f'iteration_count: {iteration_count}, min_v: {min_v}')\n",
    "        #         break\n",
    "        \n",
    "        # remaining_iterations = iterations - iteration_count\n",
    "\n",
    "        for _ in range(iterations):\n",
    "            leaf = self._selection(self.root)\n",
    "            if leaf.state == self.env.goal:\n",
    "                self._backpropagation(leaf, 0)\n",
    "                continue\n",
    "            if leaf.visits == 0:\n",
    "                leaf = self._expansion(leaf)\n",
    "            simulation_result = self._simulation(leaf)\n",
    "            self._backpropagation(leaf, simulation_result)\n",
    "        \n",
    "        # Calculate values for each action\n",
    "        values = []\n",
    "        for action in self.env.actions:\n",
    "            next_state = self.env.get_next_state(self.root.state, action)\n",
    "            for child in self.root.children:\n",
    "                \n",
    "                if child.state == next_state:\n",
    "                    values.append(child.value)\n",
    "                    break\n",
    "        # min_child_visits=min(self.root.children,key=lambda x:x.visits).visits\n",
    "        # for child in self.root.children:\n",
    "        #     # print(f'child.value: {child.value}, child.visits: {child.visits}')\n",
    "        #     pass\n",
    "        \n",
    "        return np.exp(values)/np.sum(np.exp(values)), max(values)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
